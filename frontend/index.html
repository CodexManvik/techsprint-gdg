<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>The Interview Mirror - AI Coach</title>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose/pose.js" crossorigin="anonymous"></script>

    <style>
        body { font-family: 'Segoe UI', sans-serif; background: #1a1a1a; color: white; display: flex; flex-direction: column; align-items: center; }
        h1 { margin-bottom: 10px; }
        .container { position: relative; width: 640px; height: 480px; margin-top: 20px; border: 3px solid #333; border-radius: 10px; overflow: hidden; }
        
        /* Video & Canvas Overlay */
        #input_video { position: absolute; width: 100%; height: 100%; object-fit: cover; transform: scaleX(-1); }
        #output_canvas { position: absolute; width: 100%; height: 100%; transform: scaleX(-1); }
        
        /* UI Overlays */
        .feedback-overlay { position: absolute; top: 10px; left: 10px; background: rgba(0,0,0,0.6); padding: 10px; border-radius: 8px; font-size: 14px; pointer-events: none;}
        .ai-chat-box { width: 640px; min-height: 100px; background: #2d2d2d; margin-top: 15px; padding: 15px; border-radius: 8px; border-left: 5px solid #007bff; }
        .controls { margin-top: 15px; display: flex; gap: 10px;}
        button { padding: 10px 20px; font-size: 16px; cursor: pointer; background: #007bff; color: white; border: none; border-radius: 5px; }
        button:disabled { background: #555; cursor: not-allowed; }
        button.recording { background: #dc3545; animation: pulse 1.5s infinite; }
        
        @keyframes pulse { 0% { opacity: 1; } 50% { opacity: 0.5; } 100% { opacity: 1; } }
        .status-dot { height: 10px; width: 10px; background-color: #bbb; border-radius: 50%; display: inline-block; margin-right: 5px; }
        .status-on { background-color: #28a745; }
    </style>
</head>
<body>

    <h1>The Interview Mirror ü§ñ</h1>
    <div><span id="socket-status" class="status-dot"></span> <span id="status-text">Initializing...</span></div>

    <div class="container">
        <video id="input_video"></video>
        <canvas id="output_canvas"></canvas>
        
        <div class="feedback-overlay">
            <div>üëÄ Eye Contact: <span id="score-eye">--</span></div>
            <div>üñê Fidget Score: <span id="score-fidget">--</span></div>
            <div>üôÇ Smile: <span id="score-smile">--</span></div>
            <div>ü§î Head Gesture: <span id="score-gesture">--</span></div>
            <hr style="border-color: #555; margin: 8px 0;">
            <div style="color: #4CAF50; font-weight: bold;">üìä POSTURE (NEW!)</div>
            <div>üìê Shoulder: <span id="score-shoulder">--</span></div>
            <div>üßç Slouch: <span id="score-slouch">--</span></div>
            <div>ü§û Arms: <span id="score-arms">--</span></div>
            <div>‚öñÔ∏è Stability: <span id="score-stability">--</span></div>
        </div>
    </div>

    <div class="ai-chat-box">
        <strong>AI Recruiter:</strong>
        <p id="ai-response">Please upload your resume to start.</p>
        <small style="color: #aaa; display:block; margin-top:5px;">You said: <span id="user-transcript">...</span></small>
    </div>

    <div style="margin-bottom: 15px; text-align: center; margin-top: 10px;">
        <input type="file" id="resume-upload" accept=".pdf" style="display: none;" onchange="uploadResume()">
        <button onclick="document.getElementById('resume-upload').click()" style="background: #6c757d;">
            üìÑ 1. Upload Resume
        </button>
        <p id="resume-status" style="font-size: 12px; color: #aaa; margin-top: 5px;"></p>
    </div>

    <div class="controls">
        <button id="start-btn" onclick="initializeSession()">üîÑ Reset Session</button>
        <button id="record-btn" onclick="toggleRecording()" disabled>üé§ Hold to Speak</button>
    </div>

    <script>
        // --- VARIABLES ---
        const videoElement = document.getElementById('input_video');
        const canvasElement = document.getElementById('output_canvas');
        const canvasCtx = canvasElement.getContext('2d');
        
        let socket;
        let sessionId = null;
        let isRecording = false;
        let mediaRecorder;
        let audioChunks = [];
        let currentLandmarks = null;
        let lastSentTime = 0;

        // --- 1. INITIALIZE SESSION ---
        async function initializeSession() {
            try {
                // Send default session parameters
                const response = await fetch("http://localhost:8000/start_interview", { 
                    method: "POST",
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        persona: "FAANG_Architect",
                        difficulty: "Intermediate", 
                        topic: "System Design",
                        resume_text: null
                    })
                });
                const data = await response.json();
                sessionId = data.session_id;
                console.log("Session ID:", sessionId);
                
                connectWebSocket(sessionId);
                document.getElementById('ai-response').innerText = "Session Started. Please upload your resume.";
                document.getElementById('record-btn').disabled = false;
            } catch (e) {
                console.error("Failed to start session:", e);
                document.getElementById('status-text').innerText = "Backend Offline";
            }
        }

        // --- 2. WEBSOCKET CONNECTION ---
        function connectWebSocket(id) {
            if (socket) socket.close();
            
            // Connect to the specific session ID endpoint
            socket = new WebSocket(`ws://localhost:8000/ws/interview/${id}`);
            
            socket.onopen = () => {
                document.getElementById('socket-status').classList.add('status-on');
                document.getElementById('status-text').innerText = "Connected & Tracking";
            };

            socket.onmessage = (event) => {
                const data = JSON.parse(event.data);
                
                // 1. Handle Live Metrics
                if (data.type === "metrics_update") {
                    updateDashboard(data.metrics);
                }

                // 2. Handle AI Response
                if (data.type === "ai_response") {
                    document.getElementById('status-text').innerText = "Connected"; 
                    if(data.reply) {
                        document.getElementById('ai-response').innerText = data.reply;
                        speakText(data.reply);
                    }
                    if(data.transcript) {
                        document.getElementById('user-transcript').innerText = data.transcript;
                    }
                }

                // 3. Handle Errors / No Speech (The Fix)
                if (data.type === "error") {
                    document.getElementById('status-text').innerText = "Connected"; // Reset status
                    alert(data.message); // Optional: Show alert or just text
                }
            };
            
            socket.onclose = () => {
                document.getElementById('socket-status').classList.remove('status-on');
                document.getElementById('status-text').innerText = "Disconnected";
            };
        }

        function speakText(text) {
            window.speechSynthesis.cancel(); // Stop previous speech
            const utterance = new SpeechSynthesisUtterance(text);
            window.speechSynthesis.speak(utterance);
        }

        async function uploadResume() {
            const fileInput = document.getElementById('resume-upload');
            const file = fileInput.files[0];
            if (!file) return;

            const formData = new FormData();
            formData.append('file', file);
            document.getElementById('resume-status').innerText = "Analyzing...";

            try {
                const response = await fetch('http://localhost:8000/upload-resume', {
                    method: 'POST',
                    body: formData
                });
                const data = await response.json();
                
                if (data.status === "success") {
                    document.getElementById('resume-status').innerText = "‚úÖ Resume Analyzed!";
                    document.getElementById('ai-response').innerText = data.intro;
                    speakText(data.intro);
                } else {
                    document.getElementById('resume-status').innerText = "‚ùå Error";
                }
            } catch (e) {
                console.error(e);
                document.getElementById('resume-status').innerText = "‚ùå Upload failed";
            }
        }

        function updateDashboard(metrics) {
            // Legacy metrics (from backend)
            document.getElementById('score-eye').innerText = metrics.eye_contact_score || "--";
            document.getElementById('score-fidget').innerText = metrics.fidget_score || "0";
            document.getElementById('score-smile').innerText = metrics.is_smiling ? "Yes" : "No";
            document.getElementById('score-gesture').innerText = metrics.head_gesture || "-";
            
            const eyeBox = document.getElementById('score-eye').parentElement;
            eyeBox.style.color = metrics.eye_contact_score < 0.5 ? '#ff4444' : '#fff';
            
            // Posture metrics are updated in real-time by updatePostureUI()
            // No need to update them here to avoid conflicts
        }
        
        // NEW: Draw pose skeleton on canvas (MediaPipe Pose)
        function drawPoseSkeleton(landmarks) {
            if (!landmarks || landmarks.length < 33) return;
            
            const ctx = canvasCtx;
            
            // Use MediaPipe's built-in pose connections for accuracy
            drawConnectors(ctx, landmarks, POSE_CONNECTIONS, {color: '#00FF00', lineWidth: 3});
            
            // Draw key landmark points
            const keyPoints = [0, 11, 12, 15, 16, 23, 24]; // nose, shoulders, wrists, hips
            keyPoints.forEach(i => {
                if (landmarks[i] && landmarks[i].visibility > 0.5) {
                    const lm = landmarks[i];
                    let color = '#00FF00';
                    let radius = 5;
                    
                    if (i === 0) { color = '#FF0000'; radius = 6; }  // Nose - red
                    if (i === 11 || i === 12) { color = '#FFFF00'; radius = 6; }  // Shoulders - yellow
                    if (i === 15 || i === 16) { color = '#FF00FF'; radius = 6; }  // Wrists - magenta
                    if (i === 23 || i === 24) { color = '#00FFFF'; radius = 5; }  // Hips - cyan
                    
                    ctx.fillStyle = color;
                    ctx.beginPath();
                    ctx.arc(lm.x * canvasElement.width, lm.y * canvasElement.height, radius, 0, 2 * Math.PI);
                    ctx.fill();
                    
                    // White outline
                    ctx.strokeStyle = '#FFFFFF';
                    ctx.lineWidth = 2;
                    ctx.stroke();
                }
            });
        }

        // --- DUAL MEDIAPIPE SYSTEM (FaceMesh + Pose) ---
        let currentLandmarks = null;
        let currentPoseLandmarks = null;
        let lastSentTime = 0;
        
        // FaceMesh results (for eye contact, etc.)
        function onFaceResults(results) {
            if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
                currentLandmarks = results.multiFaceLandmarks[0];
            }
        }
        
        // Pose results (for your posture analysis)
        function onPoseResults(results) {
            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
            canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);
            
            // Draw face mesh if available
            if (currentLandmarks) {
                drawConnectors(canvasCtx, currentLandmarks, FACEMESH_TESSELATION, {color: '#C0C0C070', lineWidth: 1});
                drawConnectors(canvasCtx, currentLandmarks, FACEMESH_RIGHT_EYE, {color: '#FF3030'});
                drawConnectors(canvasCtx, currentLandmarks, FACEMESH_LEFT_EYE, {color: '#30FF30'});
            }
            
            // Store pose landmarks
            currentPoseLandmarks = results.poseLandmarks;
            
            // Draw pose skeleton if available
            if (results.poseLandmarks) {
                drawPoseSkeleton(results.poseLandmarks);
                
                // Process posture analysis locally
                const postureMetrics = analyzePostureLocally(results.poseLandmarks);
                
                // Update UI immediately
                updatePostureUI(postureMetrics);
                
                // Send metrics to backend
                const now = Date.now();
                if (socket && socket.readyState === WebSocket.OPEN && !isRecording && (now - lastSentTime > 200)) {
                    socket.send(JSON.stringify({
                        type: "tracking",
                        landmarks: currentLandmarks,  // Face landmarks for legacy compatibility
                        pose_landmarks: results.poseLandmarks,  // Pose landmarks for posture
                        posture_metrics: postureMetrics  // Pre-computed posture metrics
                    }));
                    lastSentTime = now;
                }
            } else {
                // No pose detected - still send face landmarks
                const now = Date.now();
                if (socket && socket.readyState === WebSocket.OPEN && !isRecording && (now - lastSentTime > 200)) {
                    socket.send(JSON.stringify({
                        type: "tracking",
                        landmarks: currentLandmarks || []
                    }));
                    lastSentTime = now;
                }
            }
            
            canvasCtx.restore();
        }
        
        // Update posture UI immediately
        function updatePostureUI(postureMetrics) {
            if (!postureMetrics) return;
            
            const p = postureMetrics;
            
            // Shoulder angle
            const shoulderText = p.is_leaning ? 
                `${p.shoulder_angle.toFixed(1)}¬∞ ‚ö†Ô∏è LEANING` : 
                `${p.shoulder_angle.toFixed(1)}¬∞ ‚úì`;
            document.getElementById('score-shoulder').innerText = shoulderText;
            document.getElementById('score-shoulder').style.color = p.is_leaning ? '#ff4444' : '#4CAF50';
            
            // Slouch
            const slouchText = p.is_slouching ? 
                `‚ö†Ô∏è SLOUCHING (${(p.slouch_score * 100).toFixed(0)}%)` : 
                '‚úì Good';
            document.getElementById('score-slouch').innerText = slouchText;
            document.getElementById('score-slouch').style.color = p.is_slouching ? '#ff4444' : '#4CAF50';
            
            // Arms crossed
            const armsText = p.arms_crossed ? '‚ùå CROSSED' : '‚úì Open';
            document.getElementById('score-arms').innerText = armsText;
            document.getElementById('score-arms').style.color = p.arms_crossed ? '#ff4444' : '#4CAF50';
            
            // Stability
            const stabilityText = `${(p.shoulder_stability * 100).toFixed(0)}%`;
            document.getElementById('score-stability').innerText = stabilityText;
            document.getElementById('score-stability').style.color = 
                p.shoulder_stability > 0.8 ? '#4CAF50' : '#ff4444';
        }
        
        // YOUR POSTURE ANALYSIS - Running in frontend for speed!
        let shoulderHistory = [];
        let baselineNoseShoulderDist = null;
        let armsCrossedHistory = [];
        
        function analyzePostureLocally(poseLandmarks) {
            if (!poseLandmarks || poseLandmarks.length < 33) {
                return getDefaultPostureMetrics();
            }
            
            // Extract key landmarks (same as your backend code)
            const nose = poseLandmarks[0];
            const leftShoulder = poseLandmarks[11];
            const rightShoulder = poseLandmarks[12];
            const leftWrist = poseLandmarks[15];
            const rightWrist = poseLandmarks[16];
            const leftHip = poseLandmarks[23];
            const rightHip = poseLandmarks[24];
            
            // 1. Shoulder angle calculation
            const dx = rightShoulder.x - leftShoulder.x;
            const dy = rightShoulder.y - leftShoulder.y;
            let shoulderAngle = Math.atan2(dy, dx) * (180 / Math.PI);
            
            // Normalize to -90 to +90 range
            if (shoulderAngle > 90) shoulderAngle = 180 - shoulderAngle;
            else if (shoulderAngle < -90) shoulderAngle = -180 - shoulderAngle;
            
            const isLeaning = Math.abs(shoulderAngle) > 15.0;
            
            // 2. Slouch detection (adaptive baseline)
            const shoulderWidth = Math.abs(rightShoulder.x - leftShoulder.x);
            const shoulderAvgY = (leftShoulder.y + rightShoulder.y) / 2.0;
            const verticalDist = shoulderAvgY - nose.y;
            const normalizedDist = verticalDist / shoulderWidth;
            
            if (baselineNoseShoulderDist === null) {
                baselineNoseShoulderDist = normalizedDist;
            }
            
            const deviation = baselineNoseShoulderDist - normalizedDist;
            const slouchScore = Math.max(0, Math.min(1, deviation / 0.15));
            const isSlouching = slouchScore > 0.5;
            
            // 3. Arms crossed detection (your improved algorithm)
            const midlineX = (leftShoulder.x + rightShoulder.x) / 2.0;
            const hipY = (leftHip.y + rightHip.y) / 2.0;
            
            // Distance calculations
            const lwToRs = Math.hypot(leftWrist.x - rightShoulder.x, leftWrist.y - rightShoulder.y);
            const rwToLs = Math.hypot(rightWrist.x - leftShoulder.x, rightWrist.y - leftShoulder.y);
            const lwToLs = Math.hypot(leftWrist.x - leftShoulder.x, leftWrist.y - leftShoulder.y);
            const rwToRs = Math.hypot(rightWrist.x - rightShoulder.x, rightWrist.y - rightShoulder.y);
            
            const lwCenterDist = Math.hypot(leftWrist.x - midlineX, leftWrist.y - shoulderAvgY);
            const rwCenterDist = Math.hypot(rightWrist.x - midlineX, rightWrist.y - shoulderAvgY);
            
            const wristsInward = (lwCenterDist < 0.25 && rwCenterDist < 0.25);
            const wristsUp = (leftWrist.y < hipY && rightWrist.y < hipY);
            const crossed = (lwToRs < lwToLs && rwToLs < rwToRs && wristsInward && wristsUp);
            
            // Temporal smoothing for arms crossed
            armsCrossedHistory.push(crossed);
            if (armsCrossedHistory.length > 10) armsCrossedHistory.shift();
            const armsCrossed = armsCrossedHistory.filter(x => x).length >= 7;
            
            // 4. Stability tracking
            const shoulderMidX = (leftShoulder.x + rightShoulder.x) / 2.0;
            shoulderHistory.push(shoulderMidX);
            if (shoulderHistory.length > 20) shoulderHistory.shift();
            
            let rockingScore = 0;
            let shoulderStability = 1.0;
            if (shoulderHistory.length >= 10) {
                const mean = shoulderHistory.reduce((a, b) => a + b) / shoulderHistory.length;
                const variance = shoulderHistory.reduce((sum, x) => sum + Math.pow(x - mean, 2), 0) / shoulderHistory.length;
                const stdDev = Math.sqrt(variance);
                rockingScore = Math.min(1.0, stdDev / 0.02);
                shoulderStability = Math.max(0.0, 1.0 - rockingScore);
            }
            
            return {
                shoulder_angle: shoulderAngle,
                is_leaning: isLeaning,
                is_slouching: isSlouching,
                slouch_score: slouchScore,
                arms_crossed: armsCrossed,
                rocking_score: rockingScore,
                shoulder_stability: shoulderStability,
                timestamp: Date.now() / 1000
            };
        }
        
        function getDefaultPostureMetrics() {
            return {
                shoulder_angle: 0.0,
                is_leaning: false,
                is_slouching: false,
                slouch_score: 0.0,
                arms_crossed: false,
                rocking_score: 0.0,
                shoulder_stability: 1.0,
                timestamp: Date.now() / 1000
            };
        }
        
        // Initialize MediaPipe models
        console.log("üöÄ Initializing MediaPipe...");
        
        // FaceMesh for eye contact and facial features
        const faceMesh = new FaceMesh({locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`});
        faceMesh.setOptions({
            maxNumFaces: 1,
            refineLandmarks: true,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });
        faceMesh.onResults(onFaceResults);
        
        // Pose for your posture analysis
        const pose = new Pose({locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/pose/${file}`});
        pose.setOptions({
            modelComplexity: 1,
            smoothLandmarks: true,
            enableSegmentation: false,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });
        pose.onResults(onPoseResults);
        
        // Camera setup
        const camera = new Camera(videoElement, {
            onFrame: async () => {
                // Send frame to both models
                await faceMesh.send({image: videoElement});
                await pose.send({image: videoElement});
            },
            width: 640,
            height: 480
        });
        camera.start();

        // --- AUDIO RECORDING ---
        navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
            mediaRecorder = new MediaRecorder(stream);
            mediaRecorder.ondataavailable = event => audioChunks.push(event.data);
            mediaRecorder.onstop = async () => {
                const audioBlob = new Blob(audioChunks, { type: 'audio/wav' }); // Browser often records webm even if you say wav
                const reader = new FileReader();
                reader.readAsDataURL(audioBlob); 
                reader.onloadend = () => {
                    const base64Audio = reader.result.split(',')[1];
                    sendAudioPayload(base64Audio);
                };
                audioChunks = [];
            };
        });

        function toggleRecording() {
            if (!sessionId) { alert("Please wait for session to initialize."); return; }
            
            const btn = document.getElementById('record-btn');
            if (!isRecording) {
                isRecording = true;
                btn.innerText = "üõë Release to Send";
                btn.classList.add("recording");
                mediaRecorder.start();
                document.getElementById('status-text').innerText = "Listening...";
                // Stop speaking if AI is currently talking
                window.speechSynthesis.cancel();
            } else {
                isRecording = false;
                btn.innerText = "üé§ Hold to Speak";
                btn.classList.remove("recording");
                mediaRecorder.stop();
                document.getElementById('status-text').innerText = "Processing Answer...";
            }
        }

        function sendAudioPayload(audioBase64) {
            if(socket && socket.readyState === WebSocket.OPEN) {
                socket.send(JSON.stringify({
                    type: "conversation",
                    audio_data: audioBase64,
                    landmarks: currentLandmarks || []
                }));
            }
        }

        // Start everything automatically
        initializeSession();
    </script>
</body>
</html>